{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masks in decoder - analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// for local development\n",
    "%install-location /notebooks/language2motion.gt/swift-install\n",
    "%install-swiftpm-flags -c release\n",
    "%install '.package(path: \"/notebooks/language2motion.gt\")' Datasets TranslationModels TextModels ModelSupport SummaryWriter LangMotionModels Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TensorFlow\n",
    "import TextModels\n",
    "import TranslationModels\n",
    "import Foundation\n",
    "import FoundationXML\n",
    "import ModelSupport\n",
    "import Datasets\n",
    "import SummaryWriter\n",
    "import LangMotionModels\n",
    "import Checkpoints\n",
    "import PythonKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let plt = Python.import(\"matplotlib.pyplot\")\n",
    "let np = Python.import(\"numpy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let device = Device.defaultTFEager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let maxTextSequenceLength =  20\n",
    "let maxMotionLength =  50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let datasetSize: DatasetSize = .full\n",
    "let batchSize = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let dataURL = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/\")\n",
    "let motionDatasetURL = dataURL.appendingPathComponent(\"motion_dataset_v3.10Hz.\\(datasetSize.rawValue)plist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// instantiate text processor\n",
    "let vocabularyURL = dataURL.appendingPathComponent(\"vocab.txt\")\n",
    "let vocabulary: Vocabulary = try! Vocabulary(fromFile: vocabularyURL)\n",
    "let tokenizer: Tokenizer = BERTTokenizer(vocabulary: vocabulary, caseSensitive: false, unknownToken: \"[UNK]\", maxTokenLength: nil)\n",
    "let textProcessor = TextProcessor(vocabulary: vocabulary, tokenizer: tokenizer)\n",
    "\n",
    "/// instantiate model\n",
    "let modelSize = 128\n",
    "let config = LangMotionTransformerConfig(\n",
    "    vocabSize: vocabulary.count,\n",
    "    nbJoints: 47, // TODO: get value from dataset\n",
    "    nbMixtures: 20,\n",
    "    layerCount: 6,\n",
    "    modelSize: modelSize,\n",
    "    feedForwardSize: 512,\n",
    "    headCount: 4,\n",
    "    dropoutProbability:  0.1,\n",
    "    sentenceMaxPositionalLength: 100,\n",
    "    motionMaxPositionalLength: 500,\n",
    "    encoderSelfAttentionTemp: sqrt(Double(modelSize)),\n",
    "    decoderSourceAttentionTemp: sqrt(Double(modelSize)),\n",
    "    decoderSelfAttentionTemp: Double(modelSize)\n",
    ")\n",
    "\n",
    "let runName = \"run_51\"\n",
    "let epoch = 150\n",
    "\n",
    "let runURL = dataURL.appendingPathComponent(\"runs/Lang2motion/\\(runName)\", isDirectory: true)\n",
    "let checkpointURL = runURL.appendingPathComponent(\"checkpoints\", isDirectory: true)\n",
    "let motionsURL = runURL.appendingPathComponent(\"generated_motions\", isDirectory: true)\n",
    "try! FileManager().createDirectory(at: motionsURL, withIntermediateDirectories: true)\n",
    "\n",
    "// var model = LangMotionTransformer(config: config)\n",
    "let model = LangMotionTransformer(checkpoint: checkpointURL, config: config, name: \"model.e\\(epoch)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "/// load dataset\n",
    "print(\"\\nLoading dataset...\")\n",
    "\n",
    "var dataset = try Lang2Motion(\n",
    "    motionDatasetURL: motionDatasetURL,\n",
    "    batchSize: batchSize,\n",
    "    minMotionLength: 20,\n",
    "    maxMotionLength: 50,\n",
    "    trainTestSplit: 1.0,\n",
    "    device: device\n",
    ") { (motionSample: MotionSample) -> LangMotionBatch in    \n",
    "    let sentence = textProcessor.preprocess(sentence: motionSample.annotations[0], maxTextSequenceLength: maxTextSequenceLength)\n",
    "    let (motionPart, target) = LangMotionBatch.preprocessTargetMotion(sampleID: motionSample.sampleID, motion: motionSample.motion, maxMotionLength: maxMotionLength, shiftMaskRight: true)\n",
    "    let source = LangMotionBatch.Source(sentence: sentence, motionPart: motionPart)\n",
    "    let singleBatch = LangMotionBatch(source: source, target: target)\n",
    "    return singleBatch\n",
    "}\n",
    "\n",
    "print(\"Dataset acquired.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// func tensorShow(_ tensor: Tensor<Float>, cmapRange: Float = 6.0) {\n",
    "//     plt.figure(figsize: [5, 5])\n",
    "//     plt.imshow(tensor.makeNumpyArray(), aspect: \"auto\", cmap: \"Spectral\", vmin: -cmapRange, vmax: cmapRange)\n",
    "//     plt.show()\n",
    "// }\n",
    "\n",
    "func tensorShow(_ tensor: Tensor<Float>, cmapRange: Float = 6.0) {\n",
    "    plt.figure(figsize: [5, 5])\n",
    "    if cmapRange == 0.0 {\n",
    "        plt.imshow(tensor.makeNumpyArray()) //, aspect: \"auto\") //, cmap: \"Spectral\")\n",
    "    } else {\n",
    "        plt.imshow(tensor.makeNumpyArray(), aspect: \"auto\", cmap: \"Spectral\", vmin: -cmapRange, vmax: cmapRange)\n",
    "    }\n",
    "    plt.show()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension LangMotionTransformer {\n",
    "    public func getDecoderInput(sourceMask: Tensor<Float>, motionPart: LangMotionBatch.MotionPart, memory: Tensor<Float>) -> DecoderInput<Float> {\n",
    "        var motionPartFeatures: Tensor<Float>\n",
    "\n",
    "        // start flag, pos enc, current motion, padding with motion\n",
    "        let shape = motionPart.motion.shape\n",
    "        let (batchSize, numFrames) = (shape[0], shape[1])\n",
    "\n",
    "        // motion positional encoding\n",
    "        var motionPositionalEncodingVector = Tensor<Float>(repeating: 0.0, shape: [batchSize, numFrames, motionPositionalEncodingSize])\n",
    "        motionPositionalEncodingVector = motionPositionalEncoding(motionPositionalEncodingVector)\n",
    "        \n",
    "        // compute padding\n",
    "        let paddingSize = modelSize - (1 + motionPositionalEncodingSize + nbJoints)\n",
    "        \n",
    "        let multiplyBy = paddingSize/nbJoints + 1\n",
    "        let motionFramePadding = motionPart.motion.tiled(multiples: [1, 1, multiplyBy])[0..., 0..., 0..<paddingSize]\n",
    "\n",
    "        // stack everything together\n",
    "        let tensorStack = [motionPart.startFlag, motionPositionalEncodingVector, motionPart.motion, motionFramePadding]\n",
    "        let tmpMotionPartFeatures = Tensor<Float>(concatenating: tensorStack, alongAxis: 2)\n",
    "        motionPartFeatures = tmpMotionPartFeatures\n",
    "\n",
    "        motionPartFeatures = self.motionNorm(motionPartFeatures)\n",
    "        \n",
    "        let decoderInput = DecoderInput(sequence: motionPartFeatures, sourceMask: sourceMask, targetMask: motionPart.mask, memory: memory,\n",
    "                                        sourceAttentionTemperature: Float(self.decoderSourceAttentionTemp), selfAttentionTemperature: Float(self.decoderSelfAttentionTemp))\n",
    "        return decoderInput\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: visualize:\n",
    "// - mask(s)\n",
    "//   * encoder padding mask\n",
    "//   - decoder source padding mask\n",
    "//   - decoder self attention mask\n",
    "// - attention weights\n",
    "//   * encoder\n",
    "//   - decoder 1\n",
    "//   - decoder 2\n",
    "// - outputs\n",
    "//   - encoder output\n",
    "//   - decoder output\n",
    "// - signals that go through the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let motionSample = dataset.motionSamples[0]\n",
    "print(\"sampleID: \\(motionSample.sampleID)\")\n",
    "print(motionSample.description)\n",
    "print(motionSample.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let sentence = textProcessor.preprocess(sentence: motionSample.annotations[0], maxTextSequenceLength: maxTextSequenceLength)\n",
    "let (motionPart, target) = LangMotionBatch.preprocessTargetMotion(sampleID: motionSample.sampleID, motion: motionSample.motion, maxMotionLength: maxMotionLength, shiftMaskRight: true)\n",
    "let source = LangMotionBatch.Source(sentence: sentence, motionPart: motionPart)\n",
    "let singleBatch = LangMotionBatch(source: source, target: target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let trInput = singleBatch.source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func tensorShow2(_ tensor: Tensor<Float>) {\n",
    "    plt.imshow(tensor.makeNumpyArray(), cmap: \"Spectral\")\n",
    "    plt.show()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trInput.sentence.printSentence()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "var bigMask = trInput.sentence.mask.broadcasted(toShape: [1, 20, 20])\n",
    "let t1 = Tensor<Float>(repeating: 0.0, shape: [13, 20])\n",
    "bigMask[0, 7..<20, 0...] = t1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tensorShow2(bigMask.squeezingShape(at: 0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let bigSentence = LangMotionBatch.Sentence(tokenIds: trInput.sentence.tokenIds, mask: bigMask, tokenCount: trInput.sentence.tokenCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let encoded = model.encode(input: trInput.sentence)\n",
    "// let encoded = model.encode(input: bigSentence)\n",
    "encoded.lastLayerOutput.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorShow(encoded.lastLayerOutput[0], cmapRange: 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder attention probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.allLayerOutputs.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.allLayerOutputs[0].attentionOutput!.result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.allLayerOutputs[0].attentionOutput!.attentionScores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.allLayerOutputs[0].attentionOutput!.attentionScores.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.allLayerOutputs[0].attentionOutput!.attentionScores[0, 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.allLayerOutputs.map {tensorShow2($0.attentionOutput!.attentionProbs[0, 0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.allLayerOutputs.map {tensorShow($0.result[0], cmapRange: 0.0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.allLayerOutputs[5].result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sourceAttentionMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorShow2(trInput.sourceAttentionMask.squeezingShape(at: 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self-attention decoder mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorShow2(trInput.motionPart.mask.squeezingShape(at: 0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tensorShow2(trInput.motionPart.mask.squeezingShape(at: 0).transposed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change self-attention target mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var m1 = 1 - trInput.motionPart.mask.squeezingShape(at: 0)\n",
    "tensorShow2(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let t1 = Tensor<Float>(repeating: 0.0, shape: [12, 50])\n",
    "m1[38..<50, 0...] = t1\n",
    "tensorShow2(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let t2 = Tensor<Float>(repeating: 0.0, shape: [50, 12])\n",
    "m1[0..., 38..<50] = t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorShow2(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = m1.expandingShape(at: 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var m2 = trInput.motionPart.mask.squeezingShape(at: 0)\n",
    "tensorShow2(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trInput.motionPart.motionFlag.sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trInput.motionPart.motionFlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2[0..<38, 0..<38+1] -= 1\n",
    "tensorShow2(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = abs(m2)\n",
    "tensorShow2(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = m2.expandingShape(at: 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// let mp = trInput.motionPart\n",
    "// let mp1 = LangMotionBatch.MotionPart(motion: mp.motion, mask: m1, previousMotion: mp.previousMotion, startFlag: mp.startFlag, motionFlag: mp.motionFlag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let decoded = model.decode(sourceMask: trInput.sourceAttentionMask, motionPart: trInput.motionPart, memory: encoded.lastLayerOutput)\n",
    "// let decoded = model.decode(sourceMask: trInput.sourceAttentionMask, motionPart: mp1, memory: encoded.lastLayerOutput)\n",
    "print(decoded.allLayerOutputs.count)\n",
    "decoded.lastLayerOutput.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder source attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded.allLayerOutputs.map {tensorShow2($0.sourceAttentionOutput!.attentionProbs[0, 0])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one source attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let oneSourceScores = decoded.allLayerOutputs[0].sourceAttentionOutput!.attentionScores[0, 0]\n",
    "oneSourceScores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorShow2(oneSourceScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneSourceScores.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let oneSourceScores2 = oneSourceScores.replacing(with: Tensor(onesLike: oneSourceScores) * 0.0, where: oneSourceScores .< Tensor<Float>([0.0]))\n",
    "// let oneSourceScores2 = oneSourceScores.replacing(with: Tensor(zerosLike: oneSourceScores), where: oneSourceScores .< Tensor<Float>([0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorShow2(oneSourceScores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneSourceScores[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(oneSourceScores[10])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(softmax(oneSourceScores[10]).makeNumpyArray())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneSourceScores2[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(oneSourceScores2[10] * 5).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scale attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(oneSourceScores2[10] * 128)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(softmax(oneSourceScores2[10]).makeNumpyArray())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiply scores by factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(oneSourceScores[10] * sqrt(128))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(softmax(oneSourceScores[10] * sqrt(128)).makeNumpyArray())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let factor: Float = 1.0\n",
    "// let factor: Float = sqrt(128)\n",
    "// let factor: Float = 128\n",
    "// let factor: Float = 1000\n",
    "tensorShow2(softmax(oneSourceScores * factor, alongAxis: 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded.allLayerOutputs.map {tensorShow2($0.targetAttentionOutput!.attentionProbs[0, 0])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self-attention activations mins, maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorShow2(trInput.motionPart.mask.squeezingShape(at: 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let oneMaskProbs = decoded.allLayerOutputs[0].targetAttentionOutput!.attentionProbs[0, 0]\n",
    "oneMaskProbs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorShow2(oneMaskProbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneMaskProbs.min(alongAxes: 1)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneMaskProbs.max(alongAxes: 1)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(oneMaskProbs[20, 0...20].makeNumpyArray())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(oneMaskProbs[20].makeNumpyArray())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## multiply scores by sqrt(128)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(softmax(oneMaskScores[20] * sqrt(128)).makeNumpyArray())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# oneMaskScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let oneMaskScores = decoded.allLayerOutputs[0].targetAttentionOutput!.attentionScores[0, 0]\n",
    "oneMaskScores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorShow2(oneMaskScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneMaskScores.min(alongAxes: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneMaskScores.max(alongAxes: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let oneMaskScores2 = oneMaskScores.replacing(with: Tensor(zerosLike: oneMaskScores), where: oneMaskScores .< Tensor<Float>([0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorShow2(oneMaskScores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiply scores by factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let factor: Float = sqrt(128)\n",
    "// let factor: Float = 128\n",
    "// let factor: Float = 1\n",
    "tensorShow2(softmax(oneMaskScores * factor, alongAxis: 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let mixtureModelInput = Tensor<Float>(concatenating: decoded.allResults, alongAxis: 2)\n",
    "let transformerOutput = LangMotionTransformerOutput(preds: model.mixtureModel(mixtureModelInput), encoded: encoded, decoded: decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Loss function\n",
    "let args = LossArgs(\n",
    "        nb_joints: config.nbJoints,\n",
    "        nb_mixtures: config.nbMixtures,\n",
    "        mixture_regularizer_type: \"None\",  // [\"cv\", \"l2\", \"None\"]\n",
    "        mixture_regularizer: 0.0,\n",
    "        device: device\n",
    ")\n",
    "\n",
    "@differentiable(wrt: y_pred)\n",
    "public func normalMixtureSurrogateLoss2(y_pred: MixtureModelPreds, y_true: LangMotionBatch.Target, args: LossArgs) -> (Tensor<Float>, Tensor<Float>) {\n",
    "    // masking\n",
    "    var y_pred = y_pred.squeezed()\n",
    "    var y_true = y_true.squeezed()\n",
    "    let ids = Tensor<Int32>(rangeFrom: 0, to: Int32(y_true.stops.shape[1]), stride: 1, on: args.device)\n",
    "    let indices = ids.gathering(where: y_true.stops .!= Tensor(1, on: args.device))\n",
    "    y_pred = y_pred.gathering(atIndices: indices, alongAxis: 1)\n",
    "    y_true = y_true.gathering(atIndices: indices, alongAxis: 1)\n",
    "    \n",
    "    let loss = _normalMixtureSurrogateLoss(y_true: y_true, y_pred: y_pred, args: args)\n",
    "    let mean_loss = loss.mean()\n",
    "    return (mean_loss, loss)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (avg_loss, loss) = normalMixtureSurrogateLoss2(y_pred: transformerOutput.preds, y_true: singleBatch.target, args: args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_loss)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss.scalars)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decoder deep dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension TransformerDecoderLayer {\n",
    "    @differentiable\n",
    "    public func callAsFunction2(_ input: DecoderInput<Float>) -> Tensor<Float> {\n",
    "        // SR-11882\n",
    "        // we have to pass the input as a param in the Sublayer input because we still need to diferentiate\n",
    "        // targetMask, memory, and sourceMask\n",
    "        let selfNoDerivative = withoutDerivative(at: self)\n",
    "        let batchSize = withoutDerivative(at: input.batchSize)\n",
    "        \n",
    "        var output = input.sequence\n",
    "        \n",
    "        \n",
    "        output = self.sublayers[0].decoderForward(.init(sequence: output, decoderContext: input, activation: {\n",
    "            selfNoDerivative.selfAttention(.init(source: $0,\n",
    "                                                 target: $0,\n",
    "                                                 mask: $1.targetMask,\n",
    "                                                 batchSize: batchSize))\n",
    "        }))\n",
    "        output = self.sublayers[1].decoderForward(.init(sequence: output, decoderContext: input, activation: {\n",
    "            print(\"\\nsource attention\")\n",
    "            print(\"  source.shape: \\($0.shape)\")\n",
    "            print(\"  target.shape: \\($1.memory.shape)\")\n",
    "            print(\"  mask.shape: \\($1.sourceMask.shape)\")\n",
    "            return selfNoDerivative.sourceAttention(.init(source: $0,\n",
    "                                                   target: $1.memory,\n",
    "                                                   mask: $1.sourceMask,\n",
    "                                                   batchSize: batchSize))\n",
    "        }))\n",
    "        output = self.sublayers[2].decoderForward(.init(sequence: output, decoderContext: input, activation: {(result, _) in\n",
    "            selfNoDerivative.feedForward(result)\n",
    "        }))\n",
    "        return output\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension Decoder {\n",
    "    @differentiable\n",
    "    public func callAsFunction2(_ input: DecoderInput<Float>) -> DecoderOutput<Float> {\n",
    "        var allOutputs: [Tensor<Float>] = []\n",
    "        var transformerInput = input.sequence\n",
    "        let memoryInput = input.memory\n",
    "        \n",
    "        for layerIndex in 0..<(withoutDerivative(at: layers) { $0.count }) {\n",
    "            print(\"\\(layerIndex)\")\n",
    "            print(\"sequence:\\(transformerInput.shape), sourceMask:\\(input.sourceMask.shape), targetMask:\\(input.targetMask.shape), memory:\\(memoryInput.shape)\")\n",
    "            let layerOutput = layers[layerIndex].callAsFunction2(DecoderInput(\n",
    "                sequence: transformerInput,\n",
    "                sourceMask: input.sourceMask,\n",
    "                targetMask: input.targetMask,\n",
    "                memory: memoryInput\n",
    "            ))\n",
    "            allOutputs.append(layerOutput)\n",
    "            transformerInput = layerOutput\n",
    "        }\n",
    "        \n",
    "        return DecoderOutput<Float>(lastLayerOutput: transformerInput, allOutputs: allOutputs)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trInput.sentence.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorShow(trInput.sourceAttentionMask.squeezingShape(at: 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let decoderInput = model.getDecoderInput(sourceMask: trInput.sourceAttentionMask, motionPart: trInput.motionPart, memory: encodedMemory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let decoderOuptut = model.decoder.callAsFunction2(decoderInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create source attention mask of [bs x maxMotionLength x maxTextSequenceLength] dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motionPart.printMotionPart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.printTarget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func createSourceAttentionMask(sourceSequenceLength: Int, targetSequenceLength: Int, maxSourceSequenceLength: Int, maxTargetSequenceLength: Int) -> Tensor<Float> {\n",
    "    var mask = Tensor<Float>(zeros: [maxTargetSequenceLength, maxSourceSequenceLength])\n",
    "    let ones = Tensor<Float>(ones: [targetSequenceLength, sourceSequenceLength])\n",
    "    mask[0..<ones.shape[0], 0..<ones.shape[1]] = ones\n",
    "    return mask\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let mask = createSourceAttentionMask(sourceSequenceLength: 9, targetSequenceLength: 55, maxSourceSequenceLength: 20, maxTargetSequenceLength: 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorShow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Where to get values from:\n",
    "// sourceSequenceLength\n",
    "// targetSequenceLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create source attention mask from two 1-dim flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let sentenceMask = singleBatch.source.sentence.mask.squeezingShape(at: 1)\n",
    "sentenceMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let motionFlag = Tensor<Float>(singleBatch.source.motionPart.motionFlag)\n",
    "motionFlag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let sourceAttentionMask = sentenceMask * motionFlag.transposed()\n",
    "sourceAttentionMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorShow(sourceAttentionMask.squeezingShape(at: 0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let sentenceMask = singleBatch.source.sentence.mask.squeezingShape(at: 1)\n",
    "let motionFlag = Tensor<Float>(singleBatch.source.motionPart.motionFlag)\n",
    "let sourceAttentionMask = sentenceMask * motionFlag.transposed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check mask for batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let motionSample2 = dataset.motionSamples[1]\n",
    "print(\"sampleID: \\(motionSample2.sampleID)\")\n",
    "print(motionSample2.description)\n",
    "print(motionSample2.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let sentence2 = textProcessor.preprocess(sentence: motionSample2.annotations[0], maxTextSequenceLength: maxTextSequenceLength)\n",
    "let (motionPart2, target2) = LangMotionBatch.preprocessTargetMotion(sampleID: motionSample2.sampleID, motion: motionSample2.motion, maxMotionLength: maxMotionLength)\n",
    "let source2 = LangMotionBatch.Source(sentence: sentence2, motionPart: motionPart2)\n",
    "let singleBatch2 = LangMotionBatch(source: source2, target: target2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let batch = LangMotionBatch.reduceDataBatches([singleBatch, singleBatch2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let sourceAttentionMask = batch.data.sourceAttentionMask\n",
    "sourceAttentionMask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorShow(sourceAttentionMask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorShow(sourceAttentionMask[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
