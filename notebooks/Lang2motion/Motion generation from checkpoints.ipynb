{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion generation from checkpoints"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// To run in VSCode: Open notebook in jupyter lab and run installation cell in there first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// for local development\n",
    "%install-location /notebooks/language2motion.gt/swift-install\n",
    "%install-swiftpm-flags -c release\n",
    "%install '.package(path: \"/notebooks/language2motion.gt\")' Datasets TranslationModels TextModels ModelSupport SummaryWriter LangMotionModels Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TensorFlow\n",
    "import TextModels\n",
    "import TranslationModels\n",
    "import Foundation\n",
    "import FoundationXML\n",
    "import ModelSupport\n",
    "import Datasets\n",
    "import SummaryWriter\n",
    "import LangMotionModels\n",
    "import Checkpoints\n",
    "import PythonKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let np  = Python.import(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let device = Device.defaultTFEager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let maxTextSequenceLength =  20\n",
    "let maxMotionLength =  100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let datasetSize: DatasetSize = .full\n",
    "let batchSize = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let dataURL = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/\")\n",
    "let motionDatasetURL = dataURL.appendingPathComponent(\"motion_dataset_v3.10Hz.\\(datasetSize.rawValue)plist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// instantiate text processor\n",
    "let vocabularyURL = dataURL.appendingPathComponent(\"vocab.txt\")\n",
    "let vocabulary: Vocabulary = try! Vocabulary(fromFile: vocabularyURL)\n",
    "let tokenizer: Tokenizer = BERTTokenizer(vocabulary: vocabulary, caseSensitive: false, unknownToken: \"[UNK]\", maxTokenLength: nil)\n",
    "let textProcessor = TextProcessor(vocabulary: vocabulary, tokenizer: tokenizer)\n",
    "\n",
    "// model config\n",
    "let config = LangMotionTransformerConfig(\n",
    "    vocabSize: vocabulary.count,\n",
    "    nbJoints: 47, // TODO: get value from dataset\n",
    "    nbMixtures: 20,\n",
    "    layerCount: 6,\n",
    "    modelSize: 256,\n",
    "    feedForwardSize: 1024,\n",
    "    headCount: 8,\n",
    "    dropoutProbability: 0.1,\n",
    "    sentenceMaxPositionalLength: 100, \n",
    "    motionMaxPositionalLength: 500,\n",
    "    doMotionDense: false\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nLoading dataset...\")\n",
    "\n",
    "var dataset = try Lang2Motion(\n",
    "    motionDatasetURL: motionDatasetURL,\n",
    "    batchSize: batchSize,\n",
    "    minMotionLength: 10,\n",
    "    maxMotionLength: 100,\n",
    "    trainTestSplit: 1.0,\n",
    "    device: device\n",
    ") { (motionSample: MotionSample) -> LangMotionBatch in    \n",
    "    let sentence = textProcessor.preprocess(sentence: motionSample.annotations[0], maxTextSequenceLength: maxTextSequenceLength)\n",
    "    let (motionPart, target) = LangMotionBatch.preprocessTargetMotion(sampleID: motionSample.sampleID, motion: motionSample.motion, maxMotionLength: maxMotionLength)\n",
    "    let source = LangMotionBatch.Source(sentence: sentence, motionPart: motionPart)\n",
    "    let singleBatch = LangMotionBatch(data: source,label: target)\n",
    "    return singleBatch\n",
    "}\n",
    "\n",
    "print(\"Dataset acquired.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public class MotionDecoder2 {\n",
    "    public static func greedyDecodeMotion(\n",
    "        sentence: LangMotionBatch.Sentence, \n",
    "        startMotion: Tensor<Float>?,\n",
    "        transformer: LangMotionTransformer, \n",
    "        nbJoints: Int, \n",
    "        nbMixtures: Int, \n",
    "        maxMotionLength: Int,\n",
    "        memoryMultiplier: Float = 1.0\n",
    "    ) -> Tensor<Float> {\n",
    "        print(\"\\nEncode:\")\n",
    "        print(\"======\")\n",
    "        let memory = transformer.encode(input: sentence) //* memoryMultiplier\n",
    "        print(\"  memory.count: \\(memory.shape)\")     \n",
    "\n",
    "        print(\"\\nGenerate:\")\n",
    "        print(\"=========\")\n",
    "\n",
    "        // start with tensor for neutral motion frame\n",
    "        let zeroMotionFrame = LangMotionBatch.zeroMotionFrame(nbJoints: nbJoints).expandingShape(at: 0)\n",
    "        var ys: Tensor<Float> = zeroMotionFrame\n",
    "        // or with supplied motion\n",
    "//         if startMotion != nil {\n",
    "//             ys = startMotion!.expandingShape(at:0)            \n",
    "//         }\n",
    "\n",
    "        print(\"ys.shape: \\(ys.shape)\")\n",
    "        \n",
    "        let maxMotionLength2 = maxMotionLength-ys.shape[1]+1\n",
    "//         print(\"maxMotionLength2: \\(maxMotionLength2)\")\n",
    "        \n",
    "        for step in 0..<maxMotionLength2 {\n",
    "            // print(\"step: \\(step)\")\n",
    "            // prepare input\n",
    "            let motionPartFlag = Tensor<Int32>(repeating: 1, shape: [1, ys.shape[1]])\n",
    "            let motionPartMask = LangMotionBatch.makeStandardMask(target: motionPartFlag, pad: 0)\n",
    "            let previous_ys = Tensor<Float>(concatenating: [zeroMotionFrame, ys], alongAxis: 1)[0..., 0...ys.shape[1]-1, 0...]\n",
    "            var motionStartFlag = Tensor<Float>(zeros: [ys.shape[1], 1]).expandingShape(at: 0) // FIXME: refactor getting motionStartFlag\n",
    "            motionStartFlag[0, 0, 0] = Tensor(1.0)\n",
    "            let motionPart = LangMotionBatch.MotionPart(motion: ys, mask: motionPartMask, previousMotion: previous_ys, startFlag: motionStartFlag)\n",
    "            \n",
    "            \n",
    "            // decode motion\n",
    "            let dedoderOutput = transformer.decode(sourceMask: sentence.mask, motionPart: motionPart, memory: memory)\n",
    "            \n",
    "//             print(\"dedoderOutput\")\n",
    "            \n",
    "            let mixtureModelInput = Tensor<Float>(concatenating: dedoderOutput.allOutputs, alongAxis: 2)\n",
    "//             print(\"mixtureModelInput.shape: \\(mixtureModelInput.shape)\")\n",
    "            let singlePreds = transformer.mixtureModel(mixtureModelInput[0...,-1].expandingShape(at: 0))\n",
    "\n",
    "            \n",
    "            // perform sampling\n",
    "            let (sampledMotion, _, _) = MotionDecoder.performNormalMixtureSampling(\n",
    "                preds: singlePreds, nb_joints: nbJoints, nb_mixtures: nbMixtures, maxMotionLength: maxMotionLength)\n",
    "            \n",
    "            // concatenate motion\n",
    "            ys = Tensor(concatenating: [ys, sampledMotion.expandingShape(at: 0)], alongAxis: 1)        \n",
    "        }\n",
    "        return ys.squeezingShape(at:0)[1...]\n",
    "    }    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct SampleMotionClip {\n",
    "    var sampleID: Int\n",
    "    var start: Int = 0\n",
    "    var length: Int = 1\n",
    "}\n",
    "\n",
    "public func getClippedMotionFrames(dataset: Lang2Motion, clipInfo: SampleMotionClip?) -> Tensor<Float>? {\n",
    "    if clipInfo != nil {\n",
    "    \n",
    "    let ms: MotionSample = dataset.motionSamples.filter { $0.sampleID == clipInfo!.sampleID } [0]\n",
    "    let clippedMotionFrames = ms.motion[clipInfo!.start..<clipInfo!.start+clipInfo!.length]\n",
    "    return clippedMotionFrames\n",
    "    } else {\n",
    "        return nil\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public func greedyDecodeMotion(dataset: Lang2Motion, model: LangMotionTransformer, sentence: String, leadingFrames: SampleMotionClip?, prefix: String = \"prefix\", saveMotion: Bool = true, memoryMultiplier: Float = 0.0, motionsURL: URL?) {\n",
    "    let startMotion: Tensor<Float>? = getClippedMotionFrames(dataset: dataset, clipInfo: leadingFrames)\n",
    "    var leadingFramesStr = \"0\"\n",
    "    if startMotion != nil {\n",
    "        leadingFramesStr = \"\\(startMotion!.shape[0])\"\n",
    "    }\n",
    "    // TODO: incorporate done/stop signal\n",
    "    Context.local.learningPhase = .inference\n",
    "    print(\"\\ngreedyDecodeMotion(sentence: \\\"\\(sentence)\\\")\")\n",
    "\n",
    "    let processedSentence = textProcessor.preprocess(sentence: sentence, maxTextSequenceLength: maxTextSequenceLength)\n",
    "    processedSentence.printSentence()\n",
    "\n",
    "    let decodedMotion = MotionDecoder2.greedyDecodeMotion(\n",
    "        sentence: processedSentence, \n",
    "        startMotion: startMotion,\n",
    "        transformer: model, \n",
    "        nbJoints: config.nbJoints, \n",
    "        nbMixtures: config.nbMixtures, \n",
    "        maxMotionLength: maxMotionLength,\n",
    "        memoryMultiplier: memoryMultiplier\n",
    "    )\n",
    "    print(\"  decodedMotion: min: \\(decodedMotion.min()), max: \\(decodedMotion.max())\")\n",
    "    let descaledMotion = dataset.scaler.inverse_transform(decodedMotion)\n",
    "    print(\"  descaledMotion.shape: \\(descaledMotion.shape)\")\n",
    "    print(\"  descaledMotion: min: \\(descaledMotion.min()), max: \\(descaledMotion.max())\")\n",
    "    var imageURL: URL? = nil\n",
    "    \n",
    "    if !saveMotion { imageURL = nil } else {\n",
    "        imageURL = motionsURL!.appendingPathComponent(\"\\(prefix).png\")\n",
    "    }\n",
    "    // use joint groupping\n",
    "    let grouppedJointsMotion = MotionSample.grouppedJoints(motion: descaledMotion, jointNames: dataset.motionSamples[0].jointNames)\n",
    "    motionToImg(url: imageURL, motion: grouppedJointsMotion, motionFlag: nil, padTo: maxMotionLength, descr: \"\\(sentence), LF: \\(leadingFramesStr)\", cmapRange: 1.0)\n",
    "\n",
    "    if saveMotion {\n",
    "        print(\"Saved image: \\(imageURL!.path)\")\n",
    "        let jointNames = dataset.motionSamples[0].jointNames\n",
    "        let mmmXMLDoc = MMMWriter.getMMMXMLDoc(jointNames: jointNames, motion: descaledMotion)\n",
    "        let mmmURL = motionsURL!.appendingPathComponent(\"\\(prefix).mmm.xml\")\n",
    "        try! mmmXMLDoc.xmlData(options: XMLNode.Options.nodePrettyPrint).write(to: mmmURL)\n",
    "        print(\"Saved motion: \\(mmmURL.path)\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func showMotionSample(_ motionSample: MotionSample) {\n",
    "    let motion = motionSample.motion\n",
    "    let descaledMotion = dataset.scaler.inverse_transform(motion)\n",
    "    let sentence = \"sample_id=\\(motionSample.sampleID), ann=\\(motionSample.annotations[0])\"\n",
    "\n",
    "    print(\"motion: min: \\(motion.min()), max: \\(motion.max())\")\n",
    "    print(\"descaledMotion.shape: \\(descaledMotion.shape)\")\n",
    "    print(\"descaledMotion: min: \\(descaledMotion.min()), max: \\(descaledMotion.max())\")\n",
    "\n",
    "    // use joint groupping\n",
    "    let jointNames = dataset.motionSamples[0].jointNames\n",
    "    let grouppedJointsMotion = MotionSample.grouppedJoints(motion: descaledMotion, jointNames: dataset.motionSamples[0].jointNames)\n",
    "    motionToImg(url: nil, motion: grouppedJointsMotion, motionFlag: nil, padTo: maxMotionLength, descr: sentence, cmapRange: 1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func showMotion(motion: Tensor<Float>) {\n",
    "    let descaledMotion = dataset.scaler.inverse_transform(motion)\n",
    "    let grouppedJointsMotion = MotionSample.grouppedJoints(motion: descaledMotion, jointNames: dataset.motionSamples[0].jointNames)\n",
    "    motionToImg(url: nil, motion: grouppedJointsMotion, motionFlag: nil, padTo: maxMotionLength, descr: \"\", cmapRange: 1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func saveMotionToMMM(motion: Tensor<Float>, mmmURL: URL) {\n",
    "    let descaledMotion = dataset.scaler.inverse_transform(motion)\n",
    "    let jointNames = dataset.motionSamples[0].jointNames\n",
    "    let mmmXMLDoc = MMMWriter.getMMMXMLDoc(jointNames: jointNames, motion: descaledMotion)\n",
    "    try! mmmXMLDoc.xmlData(options: XMLNode.Options.nodePrettyPrint).write(to: mmmURL)\n",
    "    print(\"Saved motion: \\(mmmURL.path)\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model checkpoint"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let runName = \"run_36\"\n",
    "let epoch = 13"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "let runURL = dataURL.appendingPathComponent(\"runs/Lang2motion/\\(runName)\", isDirectory: true)\n",
    "let checkpointURL = runURL.appendingPathComponent(\"checkpoints\", isDirectory: true)\n",
    "let motionsURL = runURL.appendingPathComponent(\"generated_motions\", isDirectory: true)\n",
    "try! FileManager().createDirectory(at: motionsURL, withIntermediateDirectories: true)\n",
    "\n",
    "let model = LangMotionTransformer(checkpoint: checkpointURL, config: config, name: \"model.e\\(epoch)\")\n",
    "// let model = LangMotionTransformer(checkpoint: checkpointURL, config: config, name: \"model.final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode using leading motion frames"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Find suitable motion sample"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let annotations = dataset.langRecs\n",
    "annotations.count"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let search = \"guitar\"\n",
    "let filteredAnns = annotations.filter { $0.text.contains(search) }\n",
    "print(filteredAnns.count)\n",
    "let startIdx = 0\n",
    "filteredAnns[startIdx..<startIdx+10].map { (sampleID: $0.sampleID, ann: $0.text) }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Select motion sample"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let selAnn = filteredAnns[3]\n",
    "let selSampleInfo = (sampleID: selAnn.sampleID, text: selAnn.text, length: selAnn.motionSample.motion.shape[0])\n",
    "\n",
    "print(\"Selected motion sample\")\n",
    "print(selSampleInfo)\n",
    "showMotionSample(selAnn.motionSample)\n",
    "saveMotionToMMM(motion: selAnn.motionSample.motion, mmmURL: motionsURL.appendingPathComponent(\"sample.mmm.xml\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Clip motion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "let clipInfo = SampleMotionClip(sampleID: selSampleInfo.sampleID, start: 14, length: 10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "let clippedMotionFrames: Tensor<Float>? = getClippedMotionFrames(dataset: dataset, clipInfo: clipInfo)\n",
    "print(\"\\n**** \\(clipInfo) ****\\n\")\n",
    "print(\"Actual length: \\(clippedMotionFrames!.shape[0])\")\n",
    "print(\"clippedMotionFrames: min: \\(clippedMotionFrames!.min()), max: \\(clippedMotionFrames!.max())\")\n",
    "showMotion(motion: clippedMotionFrames!)\n",
    "saveMotionToMMM(motion: clippedMotionFrames!, mmmURL: motionsURL.appendingPathComponent(\"clip.mmm.xml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let runName = \"run_37\"\n",
    "let epoch = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "let runURL = dataURL.appendingPathComponent(\"runs/Lang2motion/\\(runName)\", isDirectory: true)\n",
    "let checkpointURL = runURL.appendingPathComponent(\"checkpoints\", isDirectory: true)\n",
    "let motionsURL = runURL.appendingPathComponent(\"generated_motions\", isDirectory: true)\n",
    "try! FileManager().createDirectory(at: motionsURL, withIntermediateDirectories: true)\n",
    "\n",
    "let model = LangMotionTransformer(checkpoint: checkpointURL, config: config, name: \"model.e\\(epoch)\")\n",
    "// let model = LangMotionTransformer(checkpoint: checkpointURL, config: config, name: \"model.final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var genNum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var s: String = \"\"\n",
    "var lf: SampleMotionClip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// s = \"A person is walking forwards five steps.\"\n",
    "// s = \"A person is walking forwards.\"\n",
    "// lf = SampleMotionClip(sampleID: 1, start: 26, length: 2)\n",
    "lf = nil\n",
    "\n",
    "// s = \"A person plays the guitar.\"\n",
    "// lf = SampleMotionClip(sampleID: 1417, start: 14, length: 10)\n",
    "\n",
    "// s = \"The human plays air guitar and sways ans stands still.\"\n",
    "// s = \"The human walks in the straight line.\"\n",
    "// s = \"Someone is jogging.\"\n",
    "\n",
    "// s = \"a person waves with his both arms\"\n",
    "// s = \"a person is waving his hand.\"\n",
    "s = \"A person runs.\"\n",
    "// s = \"A person kneels down.\"\n",
    "// s = \"A human walking backwards\"\n",
    "// s = \"A person walks 4 steps forward.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "greedyDecodeMotion(dataset: dataset, model: model, sentence: s, leadingFrames: lf, \n",
    "    prefix: \"epoch_\\(epoch)_motion_\\(genNum)\", \n",
    "    saveMotion: true,  memoryMultiplier: 1.0, motionsURL: motionsURL\n",
    ")\n",
    "genNum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
