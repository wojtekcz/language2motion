{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion generation from checkpoints"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// To run in VSCode: Open notebook in jupyter lab and run installation cell in there first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// for local development\n",
    "%install-location /notebooks/language2motion.gt/swift-install\n",
    "%install-swiftpm-flags -c release\n",
    "%install '.package(path: \"/notebooks/language2motion.gt\")' Datasets TranslationModels TextModels ModelSupport SummaryWriter LangMotionModels Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TensorFlow\n",
    "import TextModels\n",
    "import TranslationModels\n",
    "import Foundation\n",
    "import FoundationXML\n",
    "import ModelSupport\n",
    "import Datasets\n",
    "import SummaryWriter\n",
    "import LangMotionModels\n",
    "import Checkpoints\n",
    "import PythonKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let np  = Python.import(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let device = Device.defaultTFEager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let maxTextSequenceLength =  20\n",
    "let maxMotionLength =  50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let datasetSize: DatasetSize = .full\n",
    "let batchSize = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let dataURL = URL(fileURLWithPath: \"/notebooks/language2motion.gt/data/\")\n",
    "let motionDatasetURL = dataURL.appendingPathComponent(\"motion_dataset_v3.10Hz.\\(datasetSize.rawValue)plist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// instantiate text processor\n",
    "let vocabularyURL = dataURL.appendingPathComponent(\"vocab.txt\")\n",
    "let vocabulary: Vocabulary = try! Vocabulary(fromFile: vocabularyURL)\n",
    "let tokenizer: Tokenizer = BERTTokenizer(vocabulary: vocabulary, caseSensitive: false, unknownToken: \"[UNK]\", maxTokenLength: nil)\n",
    "let textProcessor = TextProcessor(vocabulary: vocabulary, tokenizer: tokenizer)\n",
    "\n",
    "// model config\n",
    "let modelSize = 128\n",
    "let config = LangMotionTransformerConfig(\n",
    "    vocabSize: vocabulary.count,\n",
    "    nbJoints: 47, // TODO: get value from dataset\n",
    "    nbMixtures: 20,\n",
    "    layerCount: 6,\n",
    "    modelSize: modelSize,\n",
    "    feedForwardSize: 512,\n",
    "    headCount: 4,\n",
    "    dropoutProbability:  0.1,\n",
    "    sentenceMaxPositionalLength: 100,\n",
    "    motionMaxPositionalLength: 500,\n",
    "//     encoderSelfAttentionTemp: Double(modelSize*modelSize),\n",
    "//     decoderSourceAttentionTemp: Double(modelSize*modelSize),\n",
    "    encoderSelfAttentionTemp: sqrt(Double(modelSize)),\n",
    "    decoderSourceAttentionTemp: sqrt(Double(modelSize)),\n",
    "    decoderSelfAttentionTemp: Double(modelSize)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nLoading dataset...\")\n",
    "\n",
    "var dataset = try Lang2Motion(\n",
    "    motionDatasetURL: motionDatasetURL,\n",
    "    batchSize: batchSize,\n",
    "    minMotionLength: 20,\n",
    "    maxMotionLength: 50,\n",
    "    trainTestSplit: 1.0,\n",
    "    device: device\n",
    ") { (motionSample: MotionSample) -> LangMotionBatch in    \n",
    "    let sentence = textProcessor.preprocess(sentence: motionSample.annotations[0], maxTextSequenceLength: maxTextSequenceLength)\n",
    "    let (motionPart, target) = LangMotionBatch.preprocessTargetMotion(sampleID: motionSample.sampleID, motion: motionSample.motion, maxMotionLength: maxMotionLength, shiftMaskRight: true)\n",
    "    let source = LangMotionBatch.Source(sentence: sentence, motionPart: motionPart)\n",
    "    let singleBatch = LangMotionBatch(data: source,label: target)\n",
    "    return singleBatch\n",
    "}\n",
    "\n",
    "print(\"Dataset acquired.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "public func subsequentMaskV2(size: Int) -> Tensor<Int32> {\n",
    "    let attentionShape = [1, size, size]\n",
    "    return 1 - Tensor<Int32>(ones: TensorShape(attentionShape))\n",
    "        .bandPart(subdiagonalCount: -1, superdiagonalCount: 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "public func makeStandardMaskV2(target: Tensor<Int32>, pad: Int32) -> Tensor<Float> {\n",
    "    var targetMask = Tensor(zerosLike: target)\n",
    "        .replacing(with: Tensor(onesLike: target), where: target .!= Tensor.init(pad))\n",
    "        .expandingShape(at: -2)\n",
    "    targetMask *= subsequentMaskV2(size: target.shape.last!)\n",
    "    return Tensor<Float>(targetMask)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let plt = Python.import(\"matplotlib.pyplot\")\n",
    "let np = Python.import(\"numpy\")\n",
    "\n",
    "func tensorShow2(_ tensor: Tensor<Float>) {\n",
    "    plt.imshow(tensor.makeNumpyArray(), cmap: \"Spectral\")\n",
    "    plt.show()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public class MotionDecoder2 {\n",
    "\n",
    "// extension MotionDecoder2 {\n",
    "    public static func greedyDecodeMotion2(\n",
    "        sentence: LangMotionBatch.Sentence, \n",
    "        startMotion: Tensor<Float>?,\n",
    "        transformer: LangMotionTransformer, \n",
    "        nbJoints: Int, \n",
    "        nbMixtures: Int, \n",
    "        maxMotionLength: Int,\n",
    "        memoryMultiplier: Float = 1.0,\n",
    "        showAttentionProbs: Bool = false\n",
    "    ) -> Tensor<Float> {\n",
    "        print(\"\\nEncode:\")\n",
    "        print(\"======\")\n",
    "        let encoded = transformer.encode(input: sentence)\n",
    "        \n",
    "        if showAttentionProbs {\n",
    "            encoded.allLayerOutputs.map {tensorShow2($0.attentionOutput!.attentionProbs[0, 0])}\n",
    "        }\n",
    "        \n",
    "        let memory = encoded.lastLayerOutput * memoryMultiplier\n",
    "        print(\"  memory.count: \\(memory.shape)\")     \n",
    "\n",
    "        print(\"\\nGenerate:\")\n",
    "        print(\"=========\")\n",
    "\n",
    "        // start with tensor for neutral motion frame\n",
    "        let zeroMotionFrame = LangMotionBatch.zeroMotionFrame(nbJoints: nbJoints).expandingShape(at: 0)\n",
    "        var ys: Tensor<Float> = zeroMotionFrame\n",
    "        // or with supplied motion\n",
    "        if startMotion != nil {\n",
    "            ys = Tensor<Float>(concatenating: [zeroMotionFrame, startMotion!.expandingShape(at:0)], alongAxis: 1)\n",
    "        }\n",
    "\n",
    "        print(\"ys.shape: \\(ys.shape)\")\n",
    "        \n",
    "        let maxMotionLength2 = maxMotionLength-ys.shape[1]+1\n",
    "        \n",
    "        for step in 0..<maxMotionLength2 {\n",
    "            // print(\"step: \\(step)\")\n",
    "            print(\".\", terminator:\"\")\n",
    "            // prepare input\n",
    "            let motionPartFlag = Tensor<Int32>(repeating: 1, shape: [1, ys.shape[1]])\n",
    "            // TODO: use makeSubsequentMask, b/c it doesn't do 0 padding to the right\n",
    "//             let motionPartMask = LangMotionBatch.makeSubsequentMask(target: motionPartFlag, pad: 0, shiftRight: true)\n",
    "//             let motionPartMask = LangMotionBatch.makeStandardMask(target: motionPartFlag, pad: 0, shiftRight: true)\n",
    "            //let motionPartMask = makeStandardMaskV2(target: motionPartFlag, pad: 0)\n",
    "            var motionPartMask = LangMotionBatch.makeStandardMask(target: motionPartFlag, pad: 0, shiftRight: true) // FIXME: fix target mask\n",
    "            let motionLen = Int(motionPartFlag.sum().scalar!)\n",
    "            motionPartMask[0, 0..<motionLen-1, 0..<motionLen] -= 1\n",
    "            motionPartMask = abs(motionPartMask)\n",
    "\n",
    "            var motionStartFlag = Tensor<Float>(zeros: [ys.shape[1], 1]).expandingShape(at: 0) // FIXME: refactor getting motionStartFlag\n",
    "            motionStartFlag[0, 0, 0] = Tensor(1.0)\n",
    "            let motionPart = LangMotionBatch.MotionPart(motion: ys, mask: motionPartMask, startFlag: motionStartFlag, motionFlag: motionPartFlag.expandingShape(at: 2))\n",
    "            let source = LangMotionBatch.Source(sentence: sentence, motionPart: motionPart)\n",
    "            // print(\"\\(step), sourceAttentionMask.shape: \\(source.sourceAttentionMask.shape)\")\n",
    "            // decode motion\n",
    "            let decoded = transformer.decode(sourceMask: source.sourceAttentionMask, motionPart: motionPart, memory: memory)\n",
    "            \n",
    "            if showAttentionProbs {\n",
    "                decoded.allLayerOutputs.map {tensorShow2($0.sourceAttentionOutput!.attentionProbs[0, 0])}\n",
    "                decoded.allLayerOutputs.map {tensorShow2($0.targetAttentionOutput!.attentionProbs[0, 0])}\n",
    "            }\n",
    "            \n",
    "            let mixtureModelInput = Tensor<Float>(concatenating: decoded.allResults, alongAxis: 2)\n",
    "            let mixtureModelInput2 = mixtureModelInput[0...,-1].expandingShape(at: 0)\n",
    "            let singlePreds = transformer.mixtureModel(mixtureModelInput2)\n",
    "            \n",
    "            // perform sampling\n",
    "            let (sampledMotion, _, _) = MotionDecoder.performNormalMixtureSampling(\n",
    "                preds: singlePreds, nb_joints: nbJoints, nb_mixtures: nbMixtures, maxMotionLength: maxMotionLength)\n",
    "            \n",
    "            // concatenate motion\n",
    "            ys = Tensor(concatenating: [ys, sampledMotion.expandingShape(at: 0)], alongAxis: 1)\n",
    "        }\n",
    "        print()\n",
    "        return ys.squeezingShape(at:0)[1...]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct SampleMotionClip {\n",
    "    var sampleID: Int\n",
    "    var start: Int = 0\n",
    "    var length: Int = 1\n",
    "}\n",
    "\n",
    "public func getClippedMotionFrames(dataset: Lang2Motion, clipInfo: SampleMotionClip?) -> Tensor<Float>? {\n",
    "    if clipInfo != nil {\n",
    "    \n",
    "    let ms: MotionSample = dataset.motionSamples.filter { $0.sampleID == clipInfo!.sampleID } [0]\n",
    "    let clippedMotionFrames = ms.motion[clipInfo!.start..<clipInfo!.start+clipInfo!.length]\n",
    "    return clippedMotionFrames\n",
    "    } else {\n",
    "        return nil\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public func greedyDecodeMotion2(dataset: Lang2Motion, model: LangMotionTransformer, sentence: String, leadingFrames: SampleMotionClip?, prefix: String = \"prefix\", \n",
    "                                saveMotion: Bool = true, memoryMultiplier: Float = 0.0, motionsURL: URL?, maxMotionLength: Int, showAttentionProbs: Bool = true) {\n",
    "    let startMotion: Tensor<Float>? = getClippedMotionFrames(dataset: dataset, clipInfo: leadingFrames)\n",
    "    var leadingFramesStr = \"0\"\n",
    "    if startMotion != nil {\n",
    "        leadingFramesStr = \"\\(startMotion!.shape[0])\"\n",
    "    }\n",
    "    // TODO: incorporate done/stop signal\n",
    "    Context.local.learningPhase = .inference\n",
    "    print(\"\\ngreedyDecodeMotion(sentence: \\\"\\(sentence)\\\")\")\n",
    "\n",
    "    let processedSentence = textProcessor.preprocess(sentence: sentence, maxTextSequenceLength: maxTextSequenceLength)\n",
    "    processedSentence.printSentence()\n",
    "\n",
    "    let decodedMotion = MotionDecoder2.greedyDecodeMotion2(\n",
    "        sentence: processedSentence, \n",
    "        startMotion: startMotion,\n",
    "        transformer: model, \n",
    "        nbJoints: config.nbJoints, \n",
    "        nbMixtures: config.nbMixtures, \n",
    "        maxMotionLength: maxMotionLength,\n",
    "        memoryMultiplier: memoryMultiplier,\n",
    "        showAttentionProbs: showAttentionProbs\n",
    "    )\n",
    "    print(\"  decodedMotion: min: \\(decodedMotion.min()), max: \\(decodedMotion.max())\")\n",
    "    let descaledMotion = dataset.scaler.inverse_transform(decodedMotion)\n",
    "    print(\"  descaledMotion.shape: \\(descaledMotion.shape)\")\n",
    "    print(\"  descaledMotion: min: \\(descaledMotion.min()), max: \\(descaledMotion.max())\")\n",
    "    var imageURL: URL? = nil\n",
    "    \n",
    "    if !saveMotion { imageURL = nil } else {\n",
    "        imageURL = motionsURL!.appendingPathComponent(\"\\(prefix).png\")\n",
    "    }\n",
    "    // use joint groupping\n",
    "    let grouppedJointsMotion = MotionSample.grouppedJoints(motion: descaledMotion, jointNames: dataset.motionSamples[0].jointNames)\n",
    "    motionToImg(url: imageURL, motion: grouppedJointsMotion, motionFlag: nil, padTo: maxMotionLength, descr: \"\\(sentence), LF: \\(leadingFramesStr)\", cmapRange: 1.0)\n",
    "\n",
    "    if saveMotion {\n",
    "        print(\"Saved image: \\(imageURL!.path)\")\n",
    "        let jointNames = dataset.motionSamples[0].jointNames\n",
    "        let mmmXMLDoc = MMMWriter.getMMMXMLDoc(jointNames: jointNames, motion: descaledMotion)\n",
    "        let mmmURL = motionsURL!.appendingPathComponent(\"\\(prefix).mmm.xml\")\n",
    "        try! mmmXMLDoc.xmlData(options: XMLNode.Options.nodePrettyPrint).write(to: mmmURL)\n",
    "        print(\"Saved motion: \\(mmmURL.path)\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func showMotionSample(_ motionSample: MotionSample) {\n",
    "    let motion = motionSample.motion\n",
    "    let descaledMotion = dataset.scaler.inverse_transform(motion)\n",
    "    let sentence = \"sample_id=\\(motionSample.sampleID), ann=\\(motionSample.annotations[0])\"\n",
    "\n",
    "    print(\"motion: min: \\(motion.min()), max: \\(motion.max())\")\n",
    "    print(\"descaledMotion.shape: \\(descaledMotion.shape)\")\n",
    "    print(\"descaledMotion: min: \\(descaledMotion.min()), max: \\(descaledMotion.max())\")\n",
    "\n",
    "    // use joint groupping\n",
    "    let jointNames = dataset.motionSamples[0].jointNames\n",
    "    let grouppedJointsMotion = MotionSample.grouppedJoints(motion: descaledMotion, jointNames: dataset.motionSamples[0].jointNames)\n",
    "    motionToImg(url: nil, motion: grouppedJointsMotion, motionFlag: nil, padTo: maxMotionLength, descr: sentence, cmapRange: 1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func showMotion(motion: Tensor<Float>) {\n",
    "    let descaledMotion = dataset.scaler.inverse_transform(motion)\n",
    "    let grouppedJointsMotion = MotionSample.grouppedJoints(motion: descaledMotion, jointNames: dataset.motionSamples[0].jointNames)\n",
    "    motionToImg(url: nil, motion: grouppedJointsMotion, motionFlag: nil, padTo: maxMotionLength, descr: \"\", cmapRange: 1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func saveMotionToMMM(motion: Tensor<Float>, mmmURL: URL) {\n",
    "    let descaledMotion = dataset.scaler.inverse_transform(motion)\n",
    "    let jointNames = dataset.motionSamples[0].jointNames\n",
    "    let mmmXMLDoc = MMMWriter.getMMMXMLDoc(jointNames: jointNames, motion: descaledMotion)\n",
    "    try! mmmXMLDoc.xmlData(options: XMLNode.Options.nodePrettyPrint).write(to: mmmURL)\n",
    "    print(\"Saved motion: \\(mmmURL.path)\")\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Load model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let runName = \"run_50\"\n",
    "let epoch = 88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "let runURL = dataURL.appendingPathComponent(\"runs/Lang2motion/\\(runName)\", isDirectory: true)\n",
    "let checkpointURL = runURL.appendingPathComponent(\"checkpoints\", isDirectory: true)\n",
    "let motionsURL = runURL.appendingPathComponent(\"generated_motions\", isDirectory: true)\n",
    "try! FileManager().createDirectory(at: motionsURL, withIntermediateDirectories: true)\n",
    "\n",
    "// let model = LangMotionTransformer(checkpoint: checkpointURL, config: config, name: \"model.e\\(epoch)\")\n",
    "// let model = LangMotionTransformer(checkpoint: checkpointURL, config: config, name: \"model.final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode using leading motion frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find suitable motion sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let annotations = dataset.langRecs\n",
    "annotations.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let dics = annotations[0..<3].map { [\"sampleID\": \"\\($0.sampleID)\", \"text\": $0.text] }\n",
    "dics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save annotations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let encoder = JSONEncoder()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let jsonData = try? encoder.encode(dic[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let dic = [\"2\": \"B\", \"1\": \"A\", \"3\": \"C\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(type(of:dic))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let encoder = JSONEncoder()\n",
    "if let jsonData = try? encoder.encode(dic) {\n",
    "    if let jsonString = String(data: jsonData, encoding: .utf8) {\n",
    "        print(jsonString)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let dic2 = dics[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(type(of:dic2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "let encoder = JSONEncoder()\n",
    "if let jsonData = try? encoder.encode(dics) {\n",
    "    if let jsonString = String(data: jsonData, encoding: .utf8) {\n",
    "        print(jsonString)\n",
    "        let URL()\n",
    "        jsonString.write(to: , atomically: , encoding: )\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let search = \"hand\"\n",
    "let filteredAnns = annotations.filter { $0.text.contains(search) }\n",
    "print(filteredAnns.count)\n",
    "let startIdx = 0\n",
    "filteredAnns[startIdx..<startIdx+min(10, filteredAnns.count)].map { (sampleID: $0.sampleID, ann: $0.text) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select motion sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let selAnn = filteredAnns[8]\n",
    "let selSampleInfo = (sampleID: selAnn.sampleID, text: selAnn.text, length: selAnn.motionSample.motion.shape[0])\n",
    "\n",
    "print(\"Selected motion sample\")\n",
    "print(selSampleInfo)\n",
    "showMotionSample(selAnn.motionSample)\n",
    "saveMotionToMMM(motion: selAnn.motionSample.motion, mmmURL: motionsURL.appendingPathComponent(\"sample.mmm.xml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "let clipInfo = SampleMotionClip(sampleID: selSampleInfo.sampleID, start: 5, length: 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "let clippedMotionFrames: Tensor<Float>? = getClippedMotionFrames(dataset: dataset, clipInfo: clipInfo)\n",
    "print(\"\\n**** \\(clipInfo) ****\\n\")\n",
    "print(\"Actual length: \\(clippedMotionFrames!.shape[0])\")\n",
    "print(\"clippedMotionFrames: min: \\(clippedMotionFrames!.min()), max: \\(clippedMotionFrames!.max())\")\n",
    "showMotion(motion: clippedMotionFrames!)\n",
    "saveMotionToMMM(motion: clippedMotionFrames!, mmmURL: motionsURL.appendingPathComponent(\"clip.mmm.xml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let runName = \"run_51\"\n",
    "let epoch = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "let runURL = dataURL.appendingPathComponent(\"runs/Lang2motion/\\(runName)\", isDirectory: true)\n",
    "let checkpointURL = runURL.appendingPathComponent(\"checkpoints\", isDirectory: true)\n",
    "let motionsURL = runURL.appendingPathComponent(\"generated_motions\", isDirectory: true)\n",
    "try! FileManager().createDirectory(at: motionsURL, withIntermediateDirectories: true)\n",
    "\n",
    "let model = LangMotionTransformer(checkpoint: checkpointURL, config: config, name: \"model.e\\(epoch)\")\n",
    "// let model = LangMotionTransformer(checkpoint: checkpointURL, config: config, name: \"model.final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var genNum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var s: String = \"\"\n",
    "var lf: SampleMotionClip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// s = \"A person is walking forwards five steps.\"\n",
    "s = \"A person is walking forwards.\"\n",
    "// lf = SampleMotionClip(sampleID: 1, start: 26, length: 2)\n",
    "lf = nil\n",
    "\n",
    "// s = \"A person plays the guitar.\"\n",
    "// lf = SampleMotionClip(sampleID: 1438, start: 14, length: 10)\n",
    "\n",
    "// s = \"The human plays air guitar and sways ans stands still.\"\n",
    "// s = \"The human walks in the straight line.\"\n",
    "// s = \"Someone is jogging.\"\n",
    "\n",
    "// s = \"a person waves with his both arms\"\n",
    "// s = \"a person is waving his hand.\"\n",
    "// s = \"a person waves with its right hand\"\n",
    "// s = \"a person raises his right hand\"\n",
    "// s = \"Someone raises a hand\"\n",
    "\n",
    "// s = \"A person runs.\"\n",
    "// s = \"The human is running\"\n",
    "// lf = SampleMotionClip(sampleID: 449, start: 14, length: 10)\n",
    "\n",
    "// s = \"A person kneels down.\"\n",
    "// s = \"A human walking backwards\"\n",
    "// s = \"A person walks 4 steps forward.\"\n",
    "\n",
    "// s = \"A person performs a high kick\"\n",
    "// lf = SampleMotionClip(sampleID: 610, start: 5, length: 10)\n",
    "// s = \"A person is standing up from kneeling.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "greedyDecodeMotion2(dataset: dataset, model: model, sentence: s, leadingFrames: lf, \n",
    "    prefix: \"epoch_\\(epoch)_motion_\\(genNum)\", \n",
    "    saveMotion: true, memoryMultiplier: 1.0, motionsURL: motionsURL,\n",
    "    maxMotionLength: 100, showAttentionProbs: false\n",
    ")\n",
    "genNum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
